{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7804f904-5746-4530-867d-c766f4501dea",
   "metadata": {},
   "source": [
    "# Prepare your Instruction Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd7517-70d9-4dee-9f92-1a2891caf385",
   "metadata": {},
   "source": [
    "An Instruction dataset is a list of instructions/outputs pairs that are relevant to your own domain. For instance it could be question and answers from an specific domain, problems and solution for a technical domain, or just instruction and outputs. A typical example is \"Write me a Python script to read a jsonL file and print the first 5 lines\" and the model would output something like:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "fname = \"my_file.json\"\n",
    "\n",
    "# read file from fname\n",
    "with open(fname, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[0:5])\n",
    "```\n",
    "\n",
    "So let's explore how one could do this?\n",
    "\n",
    "After grabbing a finetuned model and curated your own dataset, how do I create a dataset that has the right format to fine tune a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04c0a5-f481-4364-880d-10c254388987",
   "metadata": {},
   "source": [
    "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"alpaca_data.json\", \"r\") as f:\n",
    "    alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9618cd92-acdd-471b-9521-d55c38af8040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [{'instruction': 'Give three tips for staying healthy.',\n",
       "   'input': '',\n",
       "   'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'},\n",
       "  {'instruction': 'What are the three primary colors?',\n",
       "   'input': '',\n",
       "   'output': 'The three primary colors are red, blue, and yellow.'},\n",
       "  {'instruction': 'Describe the structure of an atom.',\n",
       "   'input': '',\n",
       "   'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alpaca), alpaca[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596e369-56aa-4721-9271-6686eed8fb35",
   "metadata": {},
   "source": [
    "So the dataset has instruction and outputs. The model is trained to predict the next token, so one option would be just to concat both, and train on that. We ideally format the prompt in a way that we make explicit where is the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024aec96-40fb-4417-8e64-060a301b0f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = alpaca[0]\n",
    "print(prompt_no_input(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037",
   "metadata": {},
   "source": [
    "Some other instruction have some context in the `input` variable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e42d41-a95a-41de-a3cc-1461d9e10ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Sort the following list in alphabetical order.',\n",
       " 'input': 'Camouflage, Furniture, Plaster',\n",
       " 'output': 'Furniture, Camouflage, Plaster'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca[232]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b795343f-0356-4689-8bc6-9ac650716c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = alpaca[232]\n",
    "print(prompt_input(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a",
   "metadata": {},
   "source": [
    "> But you are leaving the output out!!! Yes, but we can just concat that afterwards. Let's deal with the prompt now, we can add the output later with the right amount of padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f",
   "metadata": {},
   "source": [
    "And the refactored function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4038166-085c-4b10-a56a-2bee0bd62436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(row):\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34cc42-c38c-4e6f-bfb6-0f32d48aef5d",
   "metadata": {},
   "source": [
    "## Why are we doing all this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31dd88-e90e-4697-b354-b39eed0fab3c",
   "metadata": {},
   "source": [
    "Because we need to tokenize this dataset in a very particular way, if we want the model to learn to predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41703ce9-a22d-4245-9f6c-a424afd9ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [create_prompt(row) for row in alpaca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69153b-eb20-4f6d-afba-5b737d36320b",
   "metadata": {},
   "source": [
    "We need to process the targets and add the End Of String token (EOS) to the results. For LLama this is: `\"</s>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = \"</s>\"\n",
    "outputs = [f\"{row['output']}{EOS_TOKEN}\" for row in alpaca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5b21e8-3d5b-4964-be7b-faff5038f7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42190f2-20cf-4960-866b-ccb7700b5b20",
   "metadata": {},
   "source": [
    "Cool! but why we have everything separated? Let's sore the \"final\" version on a variable called `examples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdea7bc4-1ec3-451e-ab00-549aa2056800",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(prompts, outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad92d95-23d5-474c-9271-59948d5dcbb0",
   "metadata": {},
   "source": [
    "This is what the model need to see and learn =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.</s>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"example\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd",
   "metadata": {},
   "source": [
    "## We can actually already train the model! Lets train a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4162aec8-f2ba-45db-9633-817b416d4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da66cf59-fd17-4636-a678-320d911217b3",
   "metadata": {},
   "source": [
    "we will sort them by lenght, so we get as little padding as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ffa8bfc-1d3f-45c0-921e-b383c94ecc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort(key=lambda x: len(x[\"example\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5492465c-c1b8-4f04-a154-36ce3bcb6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:-10]\n",
    "eval_dataset = dataset[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484a86e-9d76-4085-bc2d-7cf21c3f9137",
   "metadata": {},
   "source": [
    "We will make a collate function that pad the shorter inputs with EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aa1f15f-d59c-4f54-8172-7612cd6b4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for DataLoaders\n",
    "def collate_fn(examples):\n",
    "    examples = [x[\"example\"] for x in examples]\n",
    "    batch_size = len(examples)\n",
    "    input_ids = tokenizer(examples, return_tensors='pt', padding=True)['input_ids']\n",
    "    batch = {'input_ids': input_ids[:, :-1], 'labels': input_ids[:, 1:]}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f342873a-29a3-4ed1-8b59-9e7f7f2a4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False, #this way we keep the lenght together\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4396cd66-ada3-46c7-930b-f2c654f385d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
       "        14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "        29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 13296,\n",
       "          345, 29871, 29947,   921, 29871, 29947, 29889,    13,    13,  2277,\n",
       "        29937, 13291, 29901,    13, 29953, 29946,     2,     2,     2,     2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(train_dataloader))\n",
    "b[\"input_ids\"][0][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28e18ad2-c141-4902-a5a4-24a1e743be35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nSolve 8 x 8.\\n\\n### Response:\\n64</s></s></s></s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(b[\"input_ids\"][0])[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0a46e93-7ec2-4365-8e50-be7bef873436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nSolve 8 x 8.\\n\\n### Response:\\n64</s></s></s></s></s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(b[\"labels\"][0])[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda468b-3898-4fdd-85c9-3151c2a6b8e8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7593db1-c0fa-4e1a-9a22-6c5cf83c1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_name, log=False):\n",
    "    \"Save pytorch model todisk and wandb\"\n",
    "    model_name = f\"{wandb.run.id}_{self.model_name}\"\n",
    "    torch.save(learn.model.state_dict(), f\"models/{self.model_name}.pth\")\n",
    "    if log:\n",
    "        at = wandb.Artifact(model_name, type=\"model\")\n",
    "        at.add_file(f\"models/{self.model_name}.pth\")\n",
    "        wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99a7dc-0654-4985-ac3f-60ecdc6f6558",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6925a62e-d85e-4c86-8867-bee3a180fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    model_id='meta-llama/Llama-2-7b-hf',\n",
    "    dataset_name=\"alpaca-gpt4\",\n",
    "    precision=\"bf16\",\n",
    "    n_freeze=24,\n",
    "    lr=1e-3,\n",
    "    epochs=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    batch_size=batch_size,\n",
    "    epoch_sz=len(train_dataloader),\n",
    "    log_every=len(train_dataloader)/5,\n",
    "    save_model=False,\n",
    "    mom=0.9,\n",
    "    gradient_checkpointing = True,\n",
    "    freeze_embed = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955b4579424342099f020e7cfe5bf6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    device_map=0,\n",
    "    # use_flash_attention_2=True,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16 if config.precision == \"bf16\" else torch.float32,\n",
    "    use_cache=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 6738.42M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79dbbb0-6dac-4f78-a862-8201088c9d57",
   "metadata": {},
   "source": [
    "Let's just train the last 8 layers of the model (Llama2-7B has 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_freeze = 24\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in model.model.layers[n_freeze:].parameters(): param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6232ce7a-b847-45c8-8ff7-e36249e7a060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 1750.14M\n"
     ]
    }
   ],
   "source": [
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "if config.freeze_embed:\n",
    "    model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "750ce64e-0088-4ca8-9bc9-60037e7110d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save more memory\n",
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24440392-9837-4cbb-873a-372f9f5aca20",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's compute some generations during training, we can sample form the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0a92fe7-3f9e-43e6-80b0-adbbd8b480ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens=90,\n",
    "    gen_config=gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd29e130-e8bc-4d08-ac41-04367f6109ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompts = [x[\"prompt\"] for x in eval_dataset[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ec41718-52c6-4335-a534-2790d03ba069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
    "    with torch.inference_mode():\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
    "        output = model.generate(tokenized_prompt, \n",
    "                            max_new_tokens=max_new_tokens, \n",
    "                            generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e6f44-388b-41a8-967d-4f05188c9a08",
   "metadata": {},
   "source": [
    "LoL 🤷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4e39c49-ccb1-4fe6-aa30-988ea5583b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Write a 500 word article about the benefits of organic food.\n",
      "\n",
      "### Response:\n",
      "I have been reading about the benefits of organic food and I have been convinced that I should be eating organic food. Organic food is better for you because it is healthier, more nutritious, and tastes better.\n",
      "\n",
      "Organic food is healthier because it has more nutrients and vitamins than conventional food. Organic food is also better for the environment because it does not use pesticides or other chemicals that can harm the environment.\n",
      "\n",
      "Organic food tastes better because it is grown in healthy soil and is not processed in any way.\n",
      "\n",
      "##\n"
     ]
    }
   ],
   "source": [
    "print(eval_prompts[0] + generate(eval_prompts[0], 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567920c0-005d-419d-98ab-4d797b243300",
   "metadata": {},
   "source": [
    "We can log a Table with those results to the project every X steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bac25c48-cb18-4560-b05c-1748e7d1adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "def prompt_table(prompts, log=False):\n",
    "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
    "    for prompt in progress_bar(prompts):\n",
    "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
    "        table.add_data(prompt, out, prompt+out, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
    "    if log:\n",
    "        wandb.log({\"predictions\":table})\n",
    "    return table\n",
    "\n",
    "def to_gpu(tensor_dict):\n",
    "    for key in tensor_dict.keys():\n",
    "        if torch.is_tensor(tensor_dict[key]):\n",
    "            tensor_dict[key] = tensor_dict[key].to('cuda')\n",
    "    return tensor_dict\n",
    "\n",
    "class TokenAccuracy:\n",
    "    \"A simple Accuracy function compatible with HF models\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.tp = 0.\n",
    "    def update(self, logits, labels):\n",
    "        logits, labels = logits.argmax(dim=-1).view(-1), labels.view(-1)\n",
    "        tp = (logits == labels).sum()\n",
    "        self.count += len(logits)\n",
    "        self.tp += tp\n",
    "        return tp / len(logits)\n",
    "    def compute(self):\n",
    "        return self.tp / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52fbc09-5d65-4265-abce-adda01d85584",
   "metadata": {},
   "source": [
    "Setup optimizer and else =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5374c44d-517b-42a0-ade6-297c0a5d18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), betas=(0.9,0.99), eps=1e-5)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(config.precision == \"fp16\")) # no-op if enabled=False\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_training_steps=config.epoch_sz,\n",
    "    num_warmup_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342333d-6a48-42e4-9b4e-88e42bb35c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcapecape\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tcapelle/minai/wandb/run-20230930_121819-4nhio145</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/alpaca_ft/runs/4nhio145' target=\"_blank\">young-valley-16</a></strong> to <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/alpaca_ft/runs/4nhio145' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/runs/4nhio145</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='41' class='' max='3250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.26% [41/3250 00:32&lt;42:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pt/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:17&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/envs/pt/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           tags=[\"baseline\"],\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "# Training\n",
    "acc = TokenAccuracy()\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(progress_bar(train_dataloader)):\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    for micro_step in range(config.gradient_accumulation_steps):\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**to_gpu(batch))\n",
    "            loss = out.loss / config.gradient_accumulation_steps\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optim)\n",
    "    scaler.update()\n",
    "    scheduler.step()\n",
    "\n",
    "    # we can log the metrics to W&B\n",
    "    wandb.log({\"loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "               \"accuracy\": acc.update(out.logits, batch[\"labels\"])})\n",
    "\n",
    "    if step%config.log_every==0 or step%config.epoch_sz==0:\n",
    "        prompt_table(eval_prompts, log=True)\n",
    "    \n",
    "# we save the model checkpoint at the end\n",
    "if config.save_model:\n",
    "    save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\")\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cf4bf-8685-4675-a8f8-1dce11a7fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's free the GPU\n",
    "model.to(\"cpu\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2162f551-419d-43bb-95fb-7d7264a59432",
   "metadata": {},
   "source": [
    "## The Right way now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df248c-a608-492b-9be9-de6a26788b03",
   "metadata": {},
   "source": [
    "We actually need to feed the model in a different way, with the right attention mask and penalising only the tokens on the answer:\n",
    "- We will mask the question tokens\n",
    "- We will put a large negative value on the labels we want to penalise\n",
    "- Pre-tokenize the dataset for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233bff20-0af8-490f-9bf4-0eee19f854a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816a3765-bc39-4b29-a261-093716cac661",
   "metadata": {},
   "source": [
    "Let's tokenize the prompt and the output separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef69e2e-e568-41e6-b95a-d64fc679d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_prompt = tokenizer.encode(example[\"prompt\"]) \n",
    "tokenized_output = tokenizer.encode(example[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8dbaca-0b15-4001-9d67-17be297b8d29",
   "metadata": {},
   "source": [
    "Now, let's set `input_ids` as the concatenation and `labels` as a copy of `input_ids` but masking the `prompt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c7330-f8b4-4f2e-9b1c-19b9ee70c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "input_ids = tokenized_prompt + tokenized_output\n",
    "labels = copy.deepcopy(input_ids)\n",
    "labels[:len(tokenized_prompt)] = [-100] * len(tokenized_prompt)  # we mask the tokens from the prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f3fb405-81af-4504-a270-c25779ee5cb1",
   "metadata": {},
   "source": [
    "So we will feed the model this `input_ids` and make it predict the `labels`, we also have to pass the corresponding attention mask so we don't attend to this tokens\n",
    "\n",
    "Let's process the dataset now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d900619-e81f-436c-b8f5-fc5884a2a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset):\n",
    "    tokenized_dataset = []\n",
    "    for example in progress_bar(dataset):\n",
    "        tokenized_prompt = tokenizer.encode(example[\"prompt\"]) \n",
    "        tokenized_output = tokenizer.encode(example[\"output\"])\n",
    "    \n",
    "        input_ids = tokenized_prompt + tokenized_output\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        labels[:len(tokenized_prompt)] = [-100] * len(tokenized_prompt)\n",
    "        tokenized_dataset.append({\"input_ids\": torch.tensor(input_ids), \"labels\": torch.tensor(labels)})\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f23f78-2ba7-4770-bb99-97a510632ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = process_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44762ad-8f72-4231-baa0-9861440a9f5e",
   "metadata": {},
   "source": [
    "> You should save this tokenized dataset to W&B and reload from the artifact to save some time!\n",
    "\n",
    "We now need to form batches, but the tokenized inputs have different sizes, so we will pad them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b4b3b-0f3e-4058-98d6-dcbed39a38d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea5a595-6f57-4ec6-9d46-2aca061c5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_tokenized(examples, pad_token=tokenizer.pad_token_id):\n",
    "    input_ids, labels = tuple([example[key] for example in examples] for key in (\"input_ids\", \"labels\"))\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=pad_token\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        attention_mask=input_ids.ne(pad_token),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbdcccf-b37d-42a7-817e-8a760d34ce13",
   "metadata": {},
   "source": [
    "So we pad labels with `-100` and the input_ids with the EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd79190d-1029-4ec5-a66c-4d002d863767",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_sample1 = {\"input_ids\":torch.tensor([1,2,3]), \"labels\":torch.tensor([-100,2,3])}\n",
    "dummy_sample2 = {\"input_ids\":torch.tensor([5,6,7,8]), \"labels\":torch.tensor([-100,-100,7,8])}\n",
    "\n",
    "collate_fn_tokenized([dummy_sample1, dummy_sample2], pad_token=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d6b57-4530-4bca-9c7a-e93fd3df1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    collate_fn=collate_fn_tokenized,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7814fda-fc7f-433b-b513-ec76cc048b51",
   "metadata": {},
   "source": [
    "Let's create a fresh model =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12eb985-7f61-4c6a-8b7b-218513234088",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    device_map=0,\n",
    "    # use_flash_attention_2=True,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16 if config.precision == \"bf16\" else torch.float32,\n",
    "    use_cache=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f4e785-fb8a-4cd6-9854-61ced5c12b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_freeze = 24\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in model.model.layers[n_freeze:].parameters(): param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77602046-c559-42b7-a2a5-561dcb3f5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0047f-d22d-4681-9bf2-2ebc09c41780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "if config.freeze_embed:\n",
    "    model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf6615-22e1-4d21-9c0c-997164ffefee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save more memory\n",
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c3639-6b6a-45c6-ad51-e8f31d49a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), betas=(0.9,0.99), eps=1e-5)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(config.precision == \"fp16\")) # no-op if enabled=False\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_training_steps=config.epoch_sz,\n",
    "    num_warmup_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb350de-280e-40b8-8540-5ff3c7548802",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           tags=[\"instruct\"],\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "# Training\n",
    "acc = TokenAccuracy()\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(progress_bar(train_dataloader)):\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    for micro_step in range(config.gradient_accumulation_steps):\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**to_gpu(batch))\n",
    "            loss = out.loss / config.gradient_accumulation_steps\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optim)\n",
    "    scaler.update()\n",
    "    scheduler.step()\n",
    "\n",
    "    # we can log the metrics to W&B\n",
    "    wandb.log({\"loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "               \"accuracy\": acc.update(out.logits, batch[\"labels\"])})\n",
    "\n",
    "    if step%config.log_every==0 or step%config.epoch_sz==0:\n",
    "        prompt_table(eval_prompts, log=True)\n",
    "    \n",
    "# we save the model checkpoint at the end\n",
    "if config.save_model:\n",
    "    save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\")\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcb9c1-17ec-4163-9ab8-3396b196bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo poweroff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54676ecb-3bbb-4fb5-82b7-92c8684cca9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
