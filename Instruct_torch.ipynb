{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7804f904-5746-4530-867d-c766f4501dea",
   "metadata": {},
   "source": [
    "# Prepare your Instruction Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd7517-70d9-4dee-9f92-1a2891caf385",
   "metadata": {},
   "source": [
    "An Instruction dataset is a list of instructions/outputs pairs that are relevant to your own domain. For instance it could be question and answers from an specific domain, problems and solution for a technical domain, or just instruction and outputs. A typical example is \"Write me a Python script to read a jsonL file and print the first 5 lines\" and the model would output something like:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "fname = \"my_file.json\"\n",
    "\n",
    "# read file from fname\n",
    "with open(fname, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[0:5])\n",
    "```\n",
    "\n",
    "So let's explore how one could do this?\n",
    "\n",
    "After grabbing a finetuned model and curated your own dataset, how do I create a dataset that has the right format to fine tune a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04c0a5-f481-4364-880d-10c254388987",
   "metadata": {},
   "source": [
    "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"alpaca_data.json\", \"r\") as f:\n",
    "    alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9618cd92-acdd-471b-9521-d55c38af8040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [{'instruction': 'Give three tips for staying healthy.',\n",
       "   'input': '',\n",
       "   'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'},\n",
       "  {'instruction': 'What are the three primary colors?',\n",
       "   'input': '',\n",
       "   'output': 'The three primary colors are red, blue, and yellow.'},\n",
       "  {'instruction': 'Describe the structure of an atom.',\n",
       "   'input': '',\n",
       "   'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alpaca), alpaca[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596e369-56aa-4721-9271-6686eed8fb35",
   "metadata": {},
   "source": [
    "So the dataset has instruction and outputs. The model is trained to predict the next token, so one option would be just to concat both, and train on that. We ideally format the prompt in a way that we make explicit where is the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024aec96-40fb-4417-8e64-060a301b0f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = alpaca[0]\n",
    "print(prompt_no_input(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037",
   "metadata": {},
   "source": [
    "Some other instruction have some context in the `input` variable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e42d41-a95a-41de-a3cc-1461d9e10ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Sort the following list in alphabetical order.',\n",
       " 'input': 'Camouflage, Furniture, Plaster',\n",
       " 'output': 'Furniture, Camouflage, Plaster'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca[232]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b795343f-0356-4689-8bc6-9ac650716c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = alpaca[232]\n",
    "print(prompt_input(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a",
   "metadata": {},
   "source": [
    "> But you are leaving the output out!!! Yes, but we can just concat that afterwards. Let's deal with the prompt now, we can add the output later with the right amount of padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f",
   "metadata": {},
   "source": [
    "And the refactored function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4038166-085c-4b10-a56a-2bee0bd62436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(row):\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34cc42-c38c-4e6f-bfb6-0f32d48aef5d",
   "metadata": {},
   "source": [
    "## Why are we doing all this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31dd88-e90e-4697-b354-b39eed0fab3c",
   "metadata": {},
   "source": [
    "Because we need to tokenize this dataset in a very particular way, if we want the model to learn to predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41703ce9-a22d-4245-9f6c-a424afd9ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [prompt(row) for row in alpaca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69153b-eb20-4f6d-afba-5b737d36320b",
   "metadata": {},
   "source": [
    "We need to process the targets and add the End Of String token (EOS) to the results. For LLama this is: `\"</s>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = \"</s>\"\n",
    "outputs = [f\"{row['output']}{EOS_TOKEN}\" for row in alpaca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5b21e8-3d5b-4964-be7b-faff5038f7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42190f2-20cf-4960-866b-ccb7700b5b20",
   "metadata": {},
   "source": [
    "Cool! but why we have everything separated? Let's sore the \"final\" version on a variable called `examples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdea7bc4-1ec3-451e-ab00-549aa2056800",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(prompts, outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad92d95-23d5-474c-9271-59948d5dcbb0",
   "metadata": {},
   "source": [
    "This is what the model need to see and learn =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"example\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd",
   "metadata": {},
   "source": [
    "## We can actually already train the model! Lets train a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4871b358251541bda7a0b72fe9bb70cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=0,\n",
    "    # use_flash_attention_2=True,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4162aec8-f2ba-45db-9633-817b416d4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da66cf59-fd17-4636-a678-320d911217b3",
   "metadata": {},
   "source": [
    "we will sort them by lenght, so we get as little padding as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ffa8bfc-1d3f-45c0-921e-b383c94ecc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort(key=lambda x: len(x[\"example\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5492465c-c1b8-4f04-a154-36ce3bcb6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:50000]\n",
    "eval_dataset = dataset[50000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484a86e-9d76-4085-bc2d-7cf21c3f9137",
   "metadata": {},
   "source": [
    "We will make a collate function that pad the shorter inputs with EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aa1f15f-d59c-4f54-8172-7612cd6b4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for DataLoaders\n",
    "def collate_fn(examples):\n",
    "    examples = [x[\"example\"] for x in examples]\n",
    "    batch_size = len(examples)\n",
    "    input_ids = tokenizer(examples, return_tensors='pt', padding=True)['input_ids']\n",
    "    batch = {'input_ids': input_ids[:, :-1], 'labels': input_ids[:, 1:]}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f342873a-29a3-4ed1-8b59-9e7f7f2a4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False, #this way we keep the lenght together\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4396cd66-ada3-46c7-930b-f2c654f385d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 13866,   338,   385, 15278,   393, 16612,   263,  3414, 29889,\n",
       "        14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,  2009,\n",
       "        29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13, 13296,\n",
       "          345, 29871, 29947,   921, 29871, 29947, 29889,    13,    13,  2277,\n",
       "        29937, 13291, 29901,    13, 29953, 29946,     2,     2,     2,     2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(train_dataloader))\n",
    "b[\"input_ids\"][0][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28e18ad2-c141-4902-a5a4-24a1e743be35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nSolve 8 x 8.\\n\\n### Response:\\n64</s></s></s></s>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(b[\"input_ids\"][0])[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0a46e93-7ec2-4365-8e50-be7bef873436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nSolve 8 x 8.\\n\\n### Response:\\n64</s></s></s></s></s>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(b[\"labels\"][0])[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083e093-28d5-4ffe-b63b-3c89a54734e6",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's compute some generations during training, we can sample form the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a80cb4c4-b6fd-4adf-824c-509f066387c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(model_id)\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens=90,\n",
    "    gen_config=gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44d5b383-515c-43fe-a990-e2b6002a49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompts = [x[\"prompt\"] for x in eval_dataset[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f50ca2a-dda1-49ea-9c80-dcbe19fe9e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
    "    with torch.inference_mode():\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
    "        output = model.generate(tokenized_prompt, \n",
    "                            max_new_tokens=max_new_tokens, \n",
    "                            generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed585b-3233-4187-9ebe-463995e5f28e",
   "metadata": {},
   "source": [
    "LoL 🤷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c1ddaba-4141-4690-861a-8ffe26b59b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the different types of conflict.\n",
      "\n",
      "### Response:\n",
      "\n",
      "There are four types of conflict.\n",
      "\n",
      "1. [Interpersonal Conflict](https://en.wikipedia.org/wiki/Interpersonal_conflict)\n",
      "2. [Intrapersonal Conflict](https://en.wikipedia.org/wiki/Intrapersonal_conflict)\n",
      "3. [Intraspecific Conflict](https://en.wikipedia.org/wiki/Intraspecific_conflict)\n",
      "4. [Interspecific Conflict](https://en.wikipedia.org/wiki/Interspecific_conflict)\n",
      "\n",
      "### Inst\n"
     ]
    }
   ],
   "source": [
    "print(eval_prompts[0] + generate(eval_prompts[0], 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb12a38-31cc-423f-a4dc-dc23f195d294",
   "metadata": {},
   "source": [
    "We can log a Table with those results to the project every X steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cc3ddbb-8ae8-4878-ad54-d7c3b7ec18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "def prompt_table(prompts, log=False):\n",
    "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
    "    for prompt in progress_bar(prompts):\n",
    "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
    "        table.add_data(prompt, out, prompt+out, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
    "    if log:\n",
    "        wandb.log({\"predictions\":table})\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda468b-3898-4fdd-85c9-3151c2a6b8e8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7593db1-c0fa-4e1a-9a22-6c5cf83c1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_name, log=False):\n",
    "    \"Save pytorch model todisk and wandb\"\n",
    "    model_name = f\"{wandb.run.id}_{self.model_name}\"\n",
    "    torch.save(learn.model.state_dict(), f\"models/{self.model_name}.pth\")\n",
    "    if log:\n",
    "        at = wandb.Artifact(model_name, type=\"model\")\n",
    "        at.add_file(f\"models/{self.model_name}.pth\")\n",
    "        wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99a7dc-0654-4985-ac3f-60ecdc6f6558",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 6738.42M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c263b5-5ce7-4c59-9c9c-3e5ac54ef7b1",
   "metadata": {},
   "source": [
    "Let's just train the last 8 layers of the model (Llama2-7B has 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_freeze = 24\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in model.model.layers[n_freeze:].parameters(): param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6232ce7a-b847-45c8-8ff7-e36249e7a060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 1750.14M\n"
     ]
    }
   ],
   "source": [
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6925a62e-d85e-4c86-8867-bee3a180fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    model_id=model_id,\n",
    "    dataset_name=\"alpaca-gpt4\",\n",
    "    precision=\"bf16\",\n",
    "    n_cut=32,\n",
    "    n_freeze=24,\n",
    "    lr=1e-3,\n",
    "    epochs=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    batch_size=batch_size,\n",
    "    epoch_sz=len(train_dataloader),\n",
    "    log_every=len(train_dataloader)/4,\n",
    "    save_model=False,\n",
    "    mom=0.9,\n",
    "    total_params=params,\n",
    "    trainable_params=trainable_params,\n",
    "    gradient_checkpointing = True,\n",
    "    freeze_embed = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "if config.freeze_embed:\n",
    "    model.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "750ce64e-0088-4ca8-9bc9-60037e7110d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save more memory\n",
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1eaca883-e41b-4dbd-a228-9cb6b96b00ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gpu(tensor_dict):\n",
    "    for key in tensor_dict.keys():\n",
    "        if torch.is_tensor(tensor_dict[key]):\n",
    "            tensor_dict[key] = tensor_dict[key].to('cuda')\n",
    "    return tensor_dict\n",
    "\n",
    "class TokenAccuracy:\n",
    "    \"A simple Accuracy function compatible with HF models\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.sum = 0.\n",
    "    def update(self, logits, labels):\n",
    "        logits = logits.argmax(dim=-1).view(-1)\n",
    "        labels = labels.view(-1)\n",
    "        sum = (logits == labels).sum()\n",
    "        self.count += len(logits)\n",
    "        self.sum += sum\n",
    "        return sum / len(logits)\n",
    "    def compute(self):\n",
    "        return self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5374c44d-517b-42a0-ade6-297c0a5d18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), betas=(0.9,0.99), eps=1e-5)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(config.precision == \"fp16\")) # no-op if enabled=False\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_training_steps=config.epoch_sz,\n",
    "    num_warmup_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342333d-6a48-42e4-9b4e-88e42bb35c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1ojbl5r4) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01445e0a4e5c488b94f0f245f1025869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.862 MB of 0.862 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>█▆▄▂▂▂▂▂▃▂▂▃▃▃▃▃▄▆▃▁▄▃▄▃▂▂▅▄▄▇▁▃▁▄▂▂▆▂█▄</td></tr><tr><td>loss</td><td>▇█▇▅▃▄▃▃▂▃▂▂▂▂▂▂▂▁▃▂▂▃▂▃▃▂▂▂▃▁▂▃▃▂▃▃▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.26302</td></tr><tr><td>loss</td><td>1.97483</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rich-thunder-7</strong> at: <a href='https://wandb.ai/capecape/alpaca_ft/runs/1ojbl5r4' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/runs/1ojbl5r4</a><br/> View job at <a href='https://wandb.ai/capecape/alpaca_ft/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMjQ0MTUwNg==/version_details/v1' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwMjQ0MTUwNg==/version_details/v1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230929_132923-1ojbl5r4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1ojbl5r4). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b335d8fe5e6a4e1e908b6235db83f881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111246499996115, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tcapelle/minai/wandb/run-20230929_133220-q9fkuzw9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/capecape/alpaca_ft/runs/q9fkuzw9' target=\"_blank\">vague-planet-8</a></strong> to <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/capecape/alpaca_ft' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/capecape/alpaca_ft/runs/q9fkuzw9' target=\"_blank\">https://wandb.ai/capecape/alpaca_ft/runs/q9fkuzw9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='78' class='' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.50% [78/3125 00:36&lt;23:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pt/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:07&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           tags=[\"baseline\"],\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "# Training\n",
    "acc = TokenAccuracy()\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(progress_bar(train_dataloader)):\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    for micro_step in range(config.gradient_accumulation_steps):\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**to_gpu(batch))\n",
    "            loss = out.loss / config.gradient_accumulation_steps\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optim)\n",
    "    scaler.update()\n",
    "    scheduler.step()\n",
    "\n",
    "    # we can log the metrics to W&B\n",
    "    wandb.log({\"loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "               \"accuracy\": acc.update(out.logits, batch[\"labels\"])})\n",
    "\n",
    "    if step % config.log_every == 0:\n",
    "        prompt_table(eval_prompts, log=True)\n",
    "    \n",
    "    # we save the model checkpoint at the end\n",
    "if config.save_model:\n",
    "    save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\")\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28afbe48-929c-4bcc-879e-aa50cf367069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50e965-6a3f-41f3-8caa-0a59e36968e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "b8bd419d-9689-4dbd-8b11-1cf43c0fcea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_no_input({\"instruction\":\"List the countries that have copper\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8f9b54b3-5691-4c4c-8137-da731b76ffe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "List the countries that have copper\n",
      "\n",
      "### Response:\n",
      "The countries that have copper are United States, Canada, Chile, Mexico, Peru, and Zambia.\n"
     ]
    }
   ],
   "source": [
    "print(prompt + generate(prompt, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a10715-2c79-4a41-b97b-ae3fa27d3f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
