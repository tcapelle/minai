{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7804f904-5746-4530-867d-c766f4501dea",
   "metadata": {},
   "source": [
    "# Prepare your Instruction Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd7517-70d9-4dee-9f92-1a2891caf385",
   "metadata": {},
   "source": [
    "An Instruction dataset is a list of instructions/outputs pairs that are relevant to your own domain. For instance it could be question and answers from an specific domain, problems and solution for a technical domain, or just instruction and outputs. A typical example is \"Write me a Python script to read a jsonL file and print the first 5 lines\" and the model would output something like:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "fname = \"my_file.json\"\n",
    "\n",
    "# read file from fname\n",
    "with open(fname, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[0:5])\n",
    "```\n",
    "\n",
    "So let's explore how one could do this?\n",
    "\n",
    "After grabbing a finetuned model and curated your own dataset, how do I create a dataset that has the right format to fine tune a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da04c0a5-f481-4364-880d-10c254388987",
   "metadata": {},
   "source": [
    "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fce67d2-3703-4042-816c-7a13ba9eab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"alpaca_data.json\", \"r\") as f:\n",
    "    alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9618cd92-acdd-471b-9521-d55c38af8040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [{'instruction': 'Give three tips for staying healthy.',\n",
       "   'input': '',\n",
       "   'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'},\n",
       "  {'instruction': 'What are the three primary colors?',\n",
       "   'input': '',\n",
       "   'output': 'The three primary colors are red, blue, and yellow.'},\n",
       "  {'instruction': 'Describe the structure of an atom.',\n",
       "   'input': '',\n",
       "   'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alpaca), alpaca[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e596e369-56aa-4721-9271-6686eed8fb35",
   "metadata": {},
   "source": [
    "So the dataset has instruction and outputs. The model is trained to predict the next token, so one option would be just to concat both, and train on that. We ideally format the prompt in a way that we make explicit where is the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024aec96-40fb-4417-8e64-060a301b0f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = alpaca[0]\n",
    "print(prompt_no_input(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037",
   "metadata": {},
   "source": [
    "Some other instruction have some context in the `input` variable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e42d41-a95a-41de-a3cc-1461d9e10ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Sort the following list in alphabetical order.',\n",
       " 'input': 'Camouflage, Furniture, Plaster',\n",
       " 'output': 'Furniture, Camouflage, Plaster'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca[232]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b795343f-0356-4689-8bc6-9ac650716c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Sort the following list in alphabetical order.\n",
      "\n",
      "### Input:\n",
      "Camouflage, Furniture, Plaster\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = alpaca[232]\n",
    "print(prompt_input(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a",
   "metadata": {},
   "source": [
    "> But you are leaving the output out!!! Yes, but we can just concat that afterwards. Let's deal with the prompt now, we can add the output later with the right amount of padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f",
   "metadata": {},
   "source": [
    "And the refactored function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4038166-085c-4b10-a56a-2bee0bd62436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(row):\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c34cc42-c38c-4e6f-bfb6-0f32d48aef5d",
   "metadata": {},
   "source": [
    "## Why are we doing all this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31dd88-e90e-4697-b354-b39eed0fab3c",
   "metadata": {},
   "source": [
    "Because we need to tokenize this dataset in a very particular way, if we want the model to learn to predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41703ce9-a22d-4245-9f6c-a424afd9ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [prompt(row) for row in alpaca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc69153b-eb20-4f6d-afba-5b737d36320b",
   "metadata": {},
   "source": [
    "We need to process the targets and add the End Of String token (EOS) to the results. For LLama this is: `\"</s>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = \"</s>\"\n",
    "outputs = [f\"{row['output']}{EOS_TOKEN}\" for row in alpaca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5b21e8-3d5b-4964-be7b-faff5038f7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42190f2-20cf-4960-866b-ccb7700b5b20",
   "metadata": {},
   "source": [
    "Cool! but why we have everything separated? Let's sore the \"final\" version on a variable called `examples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdea7bc4-1ec3-451e-ab00-549aa2056800",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(prompts, outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad92d95-23d5-474c-9271-59948d5dcbb0",
   "metadata": {},
   "source": [
    "This is what the model need to see and learn =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGive three tips for staying healthy.\\n\\n### Response:\\n1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"example\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd",
   "metadata": {},
   "source": [
    "## We can actually already train the model! Lets train a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c10f7f-2551-4aa4-aef2-29c888b57a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d407387cf5645daadd033f0a149829d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=0,\n",
    "    # use_flash_attention_2=True,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4162aec8-f2ba-45db-9633-817b416d4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da66cf59-fd17-4636-a678-320d911217b3",
   "metadata": {},
   "source": [
    "we will sort them by lenght, so we get as little padding as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ffa8bfc-1d3f-45c0-921e-b383c94ecc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sort(key=lambda x: len(x[\"example\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5492465c-c1b8-4f04-a154-36ce3bcb6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[:50000]\n",
    "eval_dataset = dataset[50000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484a86e-9d76-4085-bc2d-7cf21c3f9137",
   "metadata": {},
   "source": [
    "We will make a collate function that pad the shorter inputs with EOS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aa1f15f-d59c-4f54-8172-7612cd6b4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate function for DataLoaders\n",
    "def collate_fn(examples):\n",
    "    examples = [x[\"example\"] for x in examples]\n",
    "    batch_size = len(examples)\n",
    "    input_ids = tokenizer(examples, return_tensors='pt', padding=True)['input_ids']\n",
    "    batch = (input_ids[:, :-1], input_ids[:, 1:]) # input_ids, labels\n",
    "    # For HF style: batch = {'input_ids': input_ids[:, :-1], 'labels': input_ids[:, 1:]}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f342873a-29a3-4ed1-8b59-9e7f7f2a4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False, #this way we keep the lenght together\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4396cd66-ada3-46c7-930b-f2c654f385d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 50]), torch.Size([16, 50]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dataloader))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28e18ad2-c141-4902-a5a4-24a1e743be35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nSolve 8 x 8.\\n\\n### Response:\\n64</s></s></s></s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(xb[0])[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0a46e93-7ec2-4365-8e50-be7bef873436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nSolve 8 x 8.\\n\\n### Response:\\n64</s></s></s></s></s>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(yb[0])[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083e093-28d5-4ffe-b63b-3c89a54734e6",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Let's compute some generations during training, we can sample form the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a80cb4c4-b6fd-4adf-824c-509f066387c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(model_id)\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens=90,\n",
    "    gen_config=gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44d5b383-515c-43fe-a990-e2b6002a49cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompts = [x[\"prompt\"] for x in eval_dataset[0:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f50ca2a-dda1-49ea-9c80-dcbe19fe9e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
    "    with torch.inference_mode():\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
    "        output = model.generate(tokenized_prompt, \n",
    "                            max_new_tokens=max_new_tokens, \n",
    "                            generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed585b-3233-4187-9ebe-463995e5f28e",
   "metadata": {},
   "source": [
    "LoL 🤷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c1ddaba-4141-4690-861a-8ffe26b59b78",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_prompts[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_prompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, max_new_tokens, gen_config)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m      3\u001b[0m     tokenized_prompt \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 4\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(tokenized_prompt, \n\u001b[1;32m      5\u001b[0m                         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens, \n\u001b[1;32m      6\u001b[0m                         generation_config\u001b[38;5;241m=\u001b[39mgen_config)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;28mlen\u001b[39m(tokenized_prompt[\u001b[38;5;241m0\u001b[39m]):], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "print(eval_prompts[0] + generate(eval_prompts[0], 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb12a38-31cc-423f-a4dc-dc23f195d294",
   "metadata": {},
   "source": [
    "We can log a Table with those results to the project every X steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3ddbb-8ae8-4878-ad54-d7c3b7ec18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "def prompt_table(prompts, log=False):\n",
    "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
    "    for prompt in progress_bar(prompts):\n",
    "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
    "        table.add_data(prompt, out, prompt+out, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
    "    if log:\n",
    "        wandb.log({\"predictions\":table})\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2b89f-659d-468c-a39a-dfa4955b89b4",
   "metadata": {},
   "source": [
    "Let's create a Callback that will call this method every now and then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93dda4f-4e21-4584-b85a-0cceaedd6c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableCB(Callback):\n",
    "    \"Log model predictions `log_every` steps\"\n",
    "    def __init__(self, log_every=1, prompts=eval_prompts):\n",
    "        self.log_every = log_every\n",
    "        self.step = 0\n",
    "        self.prompts = prompts\n",
    "        \n",
    "    def after_batch(self, learn):\n",
    "        self.step += 1\n",
    "        if self.step % self.log_every == 0:\n",
    "            table = prompt_table(self.prompts, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda468b-3898-4fdd-85c9-3151c2a6b8e8",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e503046-a0ae-4491-b087-856c99d27d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minai.core import *\n",
    "from minai.core import _get_preds\n",
    "from torcheval.metrics import MulticlassAccuracy\n",
    "\n",
    "dls = DataLoaders(train_dataloader, eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7db1e7-2025-4603-b6dc-324ffefe601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a1da4-e7fe-40ec-99bd-85d9091467fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFMetricsCB(MetricsCB):\n",
    "    \"Fix Metrics to work with HF models!\"\n",
    "    def after_batch(self, learn):\n",
    "        x,y,*_ = to_cpu(learn.batch)\n",
    "        for m in self.metrics.values(): \n",
    "            preds = _get_preds(learn.preds, learn.preds_nm) # torcheval is not compatible with dicts\n",
    "            classes = preds.shape[-1]  #torcheval wants flatten tensors\n",
    "            m.update(to_cpu(preds).view(-1, classes), y.view(-1))\n",
    "        self.loss.update(to_cpu(learn.loss), weight=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7593db1-c0fa-4e1a-9a22-6c5cf83c1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCB(Callback):\n",
    "    \"Hacky W&B callback\"\n",
    "    order = MetricsCB.order+1\n",
    "    def __init__(self, model_name=None): \n",
    "        self.model_name = model_name\n",
    "        self.step = 0\n",
    "    def before_fit(self, learn):\n",
    "        if wandb.run is None:\n",
    "            raise Exception(\"You have to run fit inside a wandb run\")\n",
    "    \n",
    "    def after_batch(self, learn):\n",
    "        self.step += 1\n",
    "        m =  {k:v.compute().item() for k,v in learn.metrics.all_metrics.items()}\n",
    "        if learn.training:\n",
    "            m[\"step\"] = self.step\n",
    "            m[\"train_loss\"] = m[\"loss\"]\n",
    "        else: m[\"val_loss\"] = m[\"loss\"]\n",
    "        m.pop(\"loss\", None)\n",
    "        wandb.log(m)\n",
    "    \n",
    "    def after_fit(self, learn):\n",
    "        if self.model_name is not None:\n",
    "            model_name = f\"{wandb.run.id}_{self.model_name}\"\n",
    "            at = wandb.Artifact(model_name, type=\"model\")\n",
    "            torch.save(learn.model.state_dict(), f\"models/{self.model_name}.pth\")\n",
    "            at.add_file(f\"models/{self.model_name}.pth\")\n",
    "            wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6496e841-10fa-4840-9da8-07904ba1ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [DeviceCB(), \n",
    "       HFMetricsCB(accuracy=MulticlassAccuracy(num_classes=tokenizer.vocab_size))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99a7dc-0654-4985-ac3f-60ecdc6f6558",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7beb53-d76b-44c7-ab5d-17b870251df2",
   "metadata": {},
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c3959-854c-409e-9037-b5c87a6adce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c263b5-5ce7-4c59-9c9c-3e5ac54ef7b1",
   "metadata": {},
   "source": [
    "Let's just train the last 8 layers of the model (Llama2-7B has 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4de41e-5c10-478b-9524-f4a3119d277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_freeze = 24\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in model.model.layers[n_freeze:].parameters(): param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232ce7a-b847-45c8-8ff7-e36249e7a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925a62e-d85e-4c86-8867-bee3a180fc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace(\n",
    "    model_id=model_id,\n",
    "    dataset_name=\"alpaca-gpt4\",\n",
    "    n_cut=32,\n",
    "    n_freeze=24,\n",
    "    lr=1e-3,\n",
    "    epochs=1,\n",
    "    batch_size=batch_size,\n",
    "    epoch_sz=len(dls.train),\n",
    "    log_every = 5000,\n",
    "    mom=0.9,\n",
    "    total_params=params,\n",
    "    trainable_params=trainable_params,\n",
    "    gradient_checkpointing = True,\n",
    "    freeze_embed = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7fc941-65dc-4dee-839d-351bd019a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "if config.freeze_embed:\n",
    "    m.model.embed_tokens.weight.requires_grad_(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ce64e-0088-4ca8-9bc9-60037e7110d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save more memory\n",
    "if config.gradient_checkpointing:\n",
    "    m.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5374c44d-517b-42a0-ade6-297c0a5d18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "optim = partial(torch.optim.Adam, betas=(0.9,0.99), eps=1e-5)\n",
    "learn = MomentumLearner(m, dls, loss_func=loss_fn, lr=config.lr, \n",
    "                        cbs=cbs, preds_nm='logits', epoch_sz=config.epoch_sz, mom=config.mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342333d-6a48-42e4-9b4e-88e42bb35c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=\"alpaca_ft\", config=config, tags=[\"baseline\"]):\n",
    "    learn.fit(config.epochs, \n",
    "              cbs=[ProgressCB(plot=True), \n",
    "                   WandbCB(), \n",
    "                   TableCB(log_every=config.log_every)]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bd419d-9689-4dbd-8b11-1cf43c0fcea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_no_input({\"instruction\":\"List the countries that have copper\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b54b3-5691-4c4c-8137-da731b76ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt + generate(prompt, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a10715-2c79-4a41-b97b-ae3fa27d3f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
